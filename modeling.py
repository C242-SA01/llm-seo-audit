# -*- coding: utf-8 -*-
"""modeling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19b1FhKxf5Tx6Frdjgm46kts3ZXxeI1sr
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.stats import shapiro

# from google.colab import drive
# drive.mount('/content/drive')

#import dataset
data = pd.read_csv("./data/merged_data.csv")


data.isnull().sum()

data.head(1)

data.info()

data.describe()

data.isnull()

"""cleaning"""

data['Performance'] = pd.to_numeric(data['Performance'], errors='coerce')
data['Accessibility'] = pd.to_numeric(data['Accessibility'], errors='coerce')
data['Best Practices'] = pd.to_numeric(data['Best Practices'], errors='coerce')
data['SEO'] = pd.to_numeric(data['SEO'], errors='coerce')

data.head(5)

#mengubah nilai kolom open graph jika Tidak ditemukan atau NaN maka diubah menjadi 0 selain itu menjadi 1
data['Open Graph'] = data['Open Graph'].apply(lambda x: 0 if x == 'Tidak ditemukan' or pd.isnull(x) else 1)

#mengubah format nama kolom
data.columns = data.columns.str.replace(' ', '_')
data.columns = data.columns.str.lower()

##mengubah nilai kolom meta_robots jika Tidak ditemukan atau NaN maka diubah menjadi 0 selain itu menjadi 1
data['meta_robots'] = data['meta_robots'].apply(lambda x: 0 if x == 'Tidak ditemukan' or pd.isnull(x) else 1)

#menghapus kolom meta_title, meta_description, meta_keywords
data = data.drop(['meta_title', 'meta_description', 'meta_keywords'], axis=1)

data.info()

#mengisi missing value
print(data.isnull().sum())
# Select only numeric columns for calculating the mean
numeric_data = data.select_dtypes(include=np.number)
data[numeric_data.columns] = numeric_data.fillna(numeric_data.mean())
print("setelah handling :\n", data.isnull().sum())

# mendapatkan numerical feature
numerical_features = data.select_dtypes(include=np.number).columns.tolist()

# Hitung matriks korelasi
numerical_data = data.select_dtypes(include=np.number)
correlation_matrix = numerical_data.corr()

# Visualisasi korelasi
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Correlation Matrix")
plt.show()

# Menampilkan korelasi terhadap variabel target (misalnya 'performance')
print(correlation_matrix['structure'].sort_values(ascending=False))

important_features = ['accessibility','seo','best_practices','performance', 'h5', 'meta_robots', 'jumlah_kata' ,'structure']
for feature in important_features:
    # Menggunakan IQR untuk mendeteksi outlier
    Q1 = data[feature].quantile(0.25)
    Q3 = data[feature].quantile(0.75)
    IQR = Q3 - Q1

    # Identifikasi outlier
    outliers = ((data[feature] < (Q1 - 1.5 * IQR)) | (data[feature] > (Q3 + 1.5 * IQR)))
    print(f"Jumlah outlier di {feature}: {outliers.sum()}")

#menghapus outlier pada kolom accessibility, seo, best_practices
hapus_outlier = ['accessibility','seo','best_practices']
for feature in hapus_outlier:
  Q1 = data[feature].quantile(0.25)
  Q3 = data[feature].quantile(0.75)
  IQR = Q3 - Q1

  data = data[~((data[feature] < (Q1 - 1.5 * IQR)) | (data[feature] > (Q3 + 1.5 * IQR)))]

#winsorizing pada kolom performance, h5, jumlah_kata
win_outlier = ['performance','h5','jumlah_kata']
for feature in important_features:
    Q1 = data[feature].quantile(0.25)
    Q3 = data[feature].quantile(0.75)
    IQR = Q3 - Q1

    data[feature] = data[feature].clip(lower=(Q1 - 1.5 * IQR), upper=(Q3 + 1.5 * IQR))

# Visualisasi distribusi data
for feature in important_features:
    plt.figure(figsize=(8, 4))
    sns.histplot(data[feature], kde=True, bins=30, color='blue')
    plt.title(f'Distribution of {feature}')
    plt.xlabel(feature)
    plt.ylabel('Frequency')
    plt.show()

from scipy.stats import shapiro

for feature in important_features:
    stat, p = shapiro(data[feature])
    print(f"{feature}: Statistik = {stat:.4f}, p-value = {p:.4f}")
    if p > 0.05:
        print(f"{feature} kemungkinan berdistribusi normal.\n")
    else:
        print(f"{feature} kemungkinan TIDAK berdistribusi normal.\n")

#menangani distribusi tidak normal pada kolom accessibility, seo, best_practices, performance, h5, jumlah_kata
data['accessibility_log'] = np.log1p(data['accessibility'])  # Log Transform
data['jumlah_kata_log'] = np.log1p(data['jumlah_kata'])  # Log Transform
data['seo_sqrt'] = np.sqrt(data['seo'])  # Square Root Transform
data['seo_sqrt'] = np.sqrt(data['seo'])  # Square Root Transform
data['seo_sqrt'] = np.sqrt(data['seo'])  # Square Root Transform
data['h5_boxcox'], _ = stats.boxcox(data['h5'] + 1)        # Box-Cox Transform

for feature in important_features:
    stat, p = shapiro(data[feature])
    print(f"{feature}: Statistik = {stat:.4f}, p-value = {p:.4f}")
    if p > 0.05:
        print(f"{feature} kemungkinan berdistribusi normal.\n")
    else:
        print(f"{feature} kemungkinan TIDAK berdistribusi normal.\n")

"""#Model NonParametrik"""

important_features = ['accessibility', 'seo', 'best_practices','performance', 'h5','meta_robots', 'jumlah_kata'] #meta_robots dihilangkan
X = data[important_features]
y = data['structure']  # Target

"""##Random Forest"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Model Random Forest
rf_model = RandomForestRegressor(random_state=42) #inisiasi model
rf_model.fit(X_train, y_train)

# Prediksi
y_pred = rf_model.predict(X_test)

# Evaluasi
print("Mean Squared Error:", mean_squared_error(y_test, y_pred))
print("R-squared:", r2_score(y_test, y_pred))

"""hyperparameter tuning"""

from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestRegressor

# Parameter grid
param_grid_rf = {
    'n_estimators': [50, 100, 200],
    'max_depth': [10, 20, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Inisialisasi model dan GridSearchCV
rf_model = RandomForestRegressor(random_state=42)
grid_search_rf = GridSearchCV(estimator=rf_model, param_grid=param_grid_rf,
                              cv=5, scoring='r2', verbose=2, n_jobs=-1)

# Fit GridSearch
grid_search_rf.fit(X_train, y_train)

# Menampilkan hasil terbaik
print("Best Parameters for Random Forest:", grid_search_rf.best_params_)
print("Best R-squared for Random Forest:", grid_search_rf.best_score_)

# Fungsi untuk menerima data input
def get_new_input():
    new_data = {
        'accessibility': float(input("Masukkan nilai Accessibility: ")),
        'seo': float(input("Masukkan nilai SEO: ")),
        'best_practices': float(input("Masukkan nilai Best Practices: ")),
        'performance': float(input("Masukkan nilai Performance: ")),
        'h5': float(input("Masukkan jumlah H5: ")),
        'meta_robots': int(input("Meta Robots (1 jika ditemukan, 0 jika tidak): ")),
        'jumlah_kata': float(input("Masukkan jumlah kata: "))
    }
    return pd.DataFrame([new_data])

# #Ambil input dari user
# new_input = get_new_input()

# # Pastikan urutan kolom sesuai2
# new_input = new_input[important_features]

# # Prediksi nilai 'structure'
# predicted_structure = rf_model.predict(new_input)
# print(f"Prediksi nilai Structure: {predicted_structure[0]:.2f}")

## Prediksi pada data testing
#y_pred = rf_model.predict(X_test)

## Menampilkan hasil prediksi pertama
#print("Prediksi pertama:", y_pred[:5])

## Menampilkan data asli (y_test) untuk perbandingan
#print("Data asli (y_test) pertama:", y_test[:5].values)

# # Menyatukan hasil prediksi dengan data asli
# X_test_results = X_test.copy()
# X_test_results['actual_structure'] = y_test.values  # Nilai asli
# X_test_results['predicted_structure'] = y_pred      # Hasil prediksi

# # Tampilkan beberapa baris pertama
# print(X_test_results.head())

"""#Function model"""

import numpy as np
from sklearn.ensemble import RandomForestRegressor  # Menggunakan Regressor karena output numerik
import pickle  # Untuk menyimpan dan memuat model

# Inisialisasi model global
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)

# Fungsi untuk melatih model
def train_model(X_train, y_train):
    """
    Melatih model menggunakan data yang diberikan.
    Args:
        X_train (list): Data fitur dalam bentuk list.
        y_train (list): Label target dalam bentuk list.
    Returns:
        Trained model
    """
    global rf_model  # Gunakan model global
    rf_model.fit(np.array(X_train), np.array(y_train))  # Pastikan data berbentuk numpy array
    return rf_model

# Fungsi untuk prediksi
def predict_model(input_list):
    """
    Membuat prediksi berdasarkan input list.
    Args:
        input_list (list): Data fitur untuk prediksi.
    Returns:
        float: Prediksi hasil (misalnya nilai Structure)
    """
    global rf_model  # Gunakan model global
    # Mengubah input list menjadi numpy array dan reshaping menjadi 2D array
    input_array = np.array(input_list).reshape(1, -1)
    prediction = rf_model.predict(input_array)  # Prediksi
    return prediction[0]  # Mengembalikan hasil prediksi pertama

train_model(X_train, y_train)

input_list = [91, 92, 75, 42, 0, 1, 1179]
predict_model(input_list)